{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import torchvision \n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Check versions\n",
    "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x:x.repeat(3,8,8)),\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomRotation((90,90),expand=False, center=None, fill=0),\n",
    "\n",
    "    ]),\n",
    "     # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform= None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x:x.repeat(3,8,8)),\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomRotation((90,90),expand=False, center=None, fill=0),\n",
    "      \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See first training sample\n",
    "image, label = train_data[0]\n",
    "image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the shape of the image?\n",
    "image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples are there? \n",
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See classes\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module): \n",
    "\tdef __init__(self, num_classes=10): \n",
    "\t\t# Call the parent class's init method to initialize the base class \n",
    "\t\tsuper(AlexNet, self).__init__() \n",
    "\t\t\n",
    "\t\t# First Convolutional Layer with 11x11 filters, stride of 4, and 2 padding \n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2) \n",
    "\t\t\n",
    "\t\t# Max Pooling Layer with a kernel size of 3 and stride of 2 \n",
    "\t\tself.pool = nn.MaxPool2d(kernel_size=3, stride=2) \n",
    "\t\t\n",
    "\t\t# Second Convolutional Layer with 5x5 filters and 2 padding \n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2) \n",
    "\t\t\n",
    "\t\t# Third Convolutional Layer with 3x3 filters and 1 padding \n",
    "\t\tself.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1) \n",
    "\t\t\n",
    "\t\t# Fourth Convolutional Layer with 3x3 filters and 1 padding \n",
    "\t\tself.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1) \n",
    "\t\t\n",
    "\t\t# Fifth Convolutional Layer with 3x3 filters and 1 padding \n",
    "\t\tself.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1) \n",
    "\t\t\n",
    "\t\t# First Fully Connected Layer with 4096 output features \n",
    "\t\tself.fc1 = nn.Linear(in_features=256 * 6 * 6, out_features=4096) \n",
    "\t\t\n",
    "\t\t# Second Fully Connected Layer with 4096 output features \n",
    "\t\tself.fc2 = nn.Linear(in_features=4096, out_features=4096) \n",
    "\t\t\n",
    "\t\t# Output Layer with `num_classes` output features \n",
    "\t\tself.fc3 = nn.Linear(in_features=4096, out_features=num_classes) \n",
    "\n",
    "\tdef forward(self, x): \n",
    "\t\t# Pass the input through the first convolutional layer and ReLU activation function \n",
    "\t\tx = self.pool(F.relu(self.conv1(x))) \n",
    "\t\t\n",
    "\t\t# Pass the output of the first layer through \n",
    "\t\t# the second convolutional layer and ReLU activation function \n",
    "\t\tx = self.pool(F.relu(self.conv2(x))) \n",
    "\t\t\n",
    "\t\t# Pass the output of the second layer through \n",
    "\t\t# the third convolutional layer and ReLU activation function \n",
    "\t\tx = F.relu(self.conv3(x)) \n",
    "\t\t\n",
    "\t\t# Pass the output of the third layer through \n",
    "\t\t# the fourth convolutional layer and ReLU activation function \n",
    "\t\tx = F.relu(self.conv4(x)) \n",
    "\t\t\n",
    "\t\t# Pass the output of the fourth layer through \n",
    "\t\t# the fifth convolutional layer and ReLU activation function \n",
    "\t\tx = self.pool(F.relu(self.conv5(x))) \n",
    "\t\t\n",
    "\t\t# Reshape the output to be passed through the fully connected layers \n",
    "\t\tx = x.view(-1, 256 * 6 * 6) \n",
    "\t\t\n",
    "\t\t# Pass the output through the first fully connected layer and activation function \n",
    "\t\tx = F.relu(self.fc1(x)) \n",
    "\t\tx = F.dropout(x, 0.5)\t \n",
    "\t\t\n",
    "\t\t# Pass the output of the first fully connected layer through \n",
    "\t\t# the second fully connected layer and activation function \n",
    "\t\tx = F.relu(self.fc2(x)) \n",
    "\t\t\n",
    "\t\t# Pass the output of the second fully connected layer through the output layer \n",
    "\t\tx = self.fc3(x) \n",
    "\t\t\n",
    "\t\t# Return the final output \n",
    "\t\treturn x \n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "torch.manual_seed(42)\t\t\t\t\t\t\t\n",
    "alexnet = AlexNet() \n",
    "print(alexnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from helpers_function import accuracy_fn\n",
    "\n",
    "#define loss funtion & optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(alexnet.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "trainingloss=[]\n",
    "trainingaccuracy=[]\n",
    "testloss=[]\n",
    "testaccuracy=[]\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device\n",
    "               ):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    # trainingloss=[]\n",
    "    # trainingaccuracy=[]\n",
    "    model.to(device)\n",
    "    torch.autograd.set_detect_anomaly(True),\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "    trainingloss.append(train_loss)\n",
    "    trainingaccuracy.append(train_acc)\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    # testloss=[]\n",
    "    # testaccuracy=[]\n",
    "    torch.autograd.set_detect_anomaly(True),\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "\n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "\n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
    "        testloss.append(test_loss)\n",
    "        testaccuracy.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from helpers_function import print_train_time\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "# Train and test model \n",
    "epochs = 5\n",
    "for epoch in (range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=alexnet, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=alexnet,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_tensor = torch.tensor(trainingloss, device=device)\n",
    "cpu_tensor = gpu_tensor.cpu()\n",
    "numpy_array = cpu_tensor.numpy()\n",
    "\n",
    "gpu_tensor1=torch.tensor(testloss,device=device)\n",
    "cpu_tensor1=gpu_tensor1.cpu()\n",
    "numpy_array1=cpu_tensor1.numpy()\n",
    "\n",
    "\n",
    "print(numpy_array)\n",
    "print(numpy_array1)\n",
    "\n",
    "plt.plot(numpy_array)\n",
    "plt.plot(numpy_array1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Test Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "gpu_tensor2 = torch.tensor(trainingaccuracy, device=device)\n",
    "cpu_tensor2 = gpu_tensor2.cpu()\n",
    "numpy_array2 = cpu_tensor2.numpy()\n",
    "\n",
    "gpu_tensor3=torch.tensor(testaccuracy,device=device)\n",
    "cpu_tensor3=gpu_tensor3.cpu()\n",
    "numpy_array3=cpu_tensor3.numpy()\n",
    "\n",
    "\n",
    "print(numpy_array2)\n",
    "print(numpy_array3)\n",
    "\n",
    "plt.plot(trainingaccuracy)\n",
    "plt.plot(testaccuracy)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training & Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move values to device\n",
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn, \n",
    "               device: torch.device = device):\n",
    "    \"\"\"Evaluates a given model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "        device (str, optional): Target device to compute on. Defaults to device.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Send data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            \n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "        \n",
    "        # Scale loss and acc\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 1 results with device-agnostic code \n",
    "model_results = eval_model(model=alexnet, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
    "    device=device\n",
    ")\n",
    "model_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
